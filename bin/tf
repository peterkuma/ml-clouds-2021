#!/usr/bin/env python3
'''Train or apply a TensorFlow CNN.

Usage: tf train <input> <input_val> <output> <output_history> [options]
       tf apply <model> <input> <y1> <y2> <output> [options]

Depends on: prepare_samples

Arguments (train):

- input: Input directory with samples - the output of prepare_samples (NetCDF).
- input_val: Input directory with validation samples (NetCDF).
- output: Output model (HDF5).
- output_history: History output (NetCDF).

Options (train):

- night: <value>: Train for nighttime only. One of: true or false.
  Default: false.
- classes: <value>: Classification. One of: 0 (4 cloud types),
  1 (10 cloud genera), 2 (27 cloud genera). Default: 0.
- inmemory: <value>: Enable in-memory training. One of: true or false.
  Default: true.

Arguments (apply):

- model: TensorFlow model (HDF5).
- input: Input directory with samples - the output of prepare_samples (NetCDF).
- y1: Start year.
- y2: End year.
- output: Output samples directory (NetCDF).

Options (apply):

- classes: <value>: Classification. One of: 0 (4 cloud types),
  1 (10 cloud genera), 2 (27 cloud genera). Default: 0.

Examples:

bin/tf train data/samples/ceres_training/training data/samples/ceres_training/validation data/ann/ceres.h5 data/ann/history.nc
bin/tf apply data/ann/ceres.h5 data/samples/ceres 2003 2020 data/samples_tf/ceres
bin/tf apply data/ann/ceres.h5 data/samples/historical/AWI-ESM-1-1-LR 2003 2014 data/samples_tf/historical/AWI-ESM-1-1-LR
'''

import warnings
warnings.filterwarnings('ignore')

import sys
import re
import os
import random
import numpy as np
import ds_format as ds
import aquarius_time as aq
import matplotlib.pyplot as plt
import pst
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Model
from keras.layers.merge import concatenate
from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, \
	AveragePooling2D, MaxPooling2D, Flatten, Dense, Dropout, \
	BatchNormalization, Activation
from tensorflow.keras.utils import Sequence

def calc_stats(samples):
	n = samples.shape[0]
	nclasses = samples.shape[-1]
	return np.mean(samples[:,2:-2,2:-2,:], axis=(1, 2))

def get_loss_func(nclasses):
	def loss_func(y_true, y_pred):
		a = y_true[:,:,:,0:nclasses]
		b = y_true[:,:,:,nclasses:(2*nclasses)]
		c = y_pred[:,:,:,0:nclasses]
		d = a*tf.experimental.numpy.log(c) + \
			(b - a)*tf.experimental.numpy.log(1 - c)
		x = -tf.experimental.numpy.sum(d)
		return x
	return loss_func

def unet_encoder(x, n, kernel_size=3, batchnorm=True, dropout=0, maxpool=True):
	y = Conv2D(n, (kernel_size, kernel_size), kernel_initializer='he_normal', \
		padding='same')(x)
	if batchnorm:
		y = BatchNormalization()(y)
	y = Activation('relu')(y)
	y = Conv2D(n, (kernel_size, kernel_size), kernel_initializer='he_normal', \
		padding='same')(x)
	if batchnorm:
		y = BatchNormalization()(y)
	y = Activation('relu')(y)
	z = y
	if maxpool:
		y = AveragePooling2D((2, 2))(y)
	if dropout > 0:
		y = Dropout(dropout)(y)
	return y, z

def unet_decoder(x, z, n, kernel_size=3, batchnorm=True, dropout=0):
	y = Conv2DTranspose(n, (kernel_size, kernel_size), strides=(2, 2),
		padding='same')(x)
	y = concatenate([y, z])
	y = Dropout(dropout)(y)
	y, _ = unet_encoder(y, n, kernel_size, batchnorm, 0, False)
	return y

def unet_model(x, nclasses, n=16, dropout=0.1, batchnorm=True):
	y, c1 = unet_encoder(x, n*1, 3, batchnorm, dropout)
	y, c2 = unet_encoder(y, n*2, 3, batchnorm, dropout)
	y, c3 = unet_encoder(y, n*4, 3, batchnorm, dropout)
	y, c4 = unet_encoder(y, n*8, 3, batchnorm, dropout)
	y, c5 = unet_encoder(y, n, 3, batchnorm, 0, False)
	y = unet_decoder(y, c4, n*8, 3, batchnorm, dropout)
	y = unet_decoder(y, c3, n*4, 3, batchnorm, dropout)
	y = unet_decoder(y, c2, n*2, 3, batchnorm, dropout)
	y = unet_decoder(y, c1, n*2, 3, batchnorm, dropout)
	y = Conv2D(nclasses*2, (1, 1), activation='sigmoid')(y)
	return Model(inputs=[x], outputs=[y])

def read_samples(filename):
	print('<- %s' % filename)
	d = ds.read(filename, ['data', 'time', 'lat', 'lon', 'station_clouds', \
		'station_clouds_n'])
	n, m, l = d['data'].shape[2], d['data'].shape[3], d['data'].shape[0]
	images = np.moveaxis(d['data'], 1, 3)
	if 'station_clouds' in d:
		nclasses = d['station_clouds'].shape[1]
		labels = np.full((l, n, m, nclasses*2), np.nan, np.float64)
		labels[:,:,:,:nclasses] = np.moveaxis(d['station_clouds'], 1, 3)
		labels[:,:,:,nclasses:] = np.moveaxis(d['station_clouds_n'], 1, 3)
		labels[np.isnan(labels)] = 0
	else:
		labels = None
	mask = np.array([
		np.sum(np.isnan(d['data'][i,:,:])) == 0
		for i in range(l)
	])
	return \
		images[mask,::], \
		labels[mask,::] if labels is not None else None, \
		d['time'][mask] if 'time' in d else None, \
		d['lat'][mask,::] if 'lat' in d else None, \
		d['lon'][mask,::] if 'lon' in d else None

def printa(label, *aa):
	print(label, end=' ')
	for a in aa:
		print(' '.join(['% 3d' % x for x in a]), end=' ')
	print()

class DataGenerator(Sequence):
	def __init__(self, input_, batch_size, from_=0, to=-1):
		self.filenames = [os.path.join(input_, file_) \
			for file_ in sorted(os.listdir(input_))[from_:to]
			if file_.endswith('.nc')
		]
		self.batch_size = batch_size

	def __getitem__(self, i):
		x = []
		y = []
		for j in range(self.batch_size):
			k = i*self.batch_size + j
			if k >= len(self.filenames):
				break
			x0, y0, _, _, _ = read_samples(self.filenames[k])
			x += [x0]
			y += [y0]
		if len(x) > 1:
			x = np.concatenate(x)
		if len(y) > 1:
			y = np.concatenate(y)
		return x, y

	def __len__(self):
		return len(self.filenames)//self.batch_size

def apply_(model_file, input_, y1, y2, output, nclasses, night=False):
	files = os.listdir(input_)
	for file_ in sorted(files):
		if not file_.endswith('.nc'):
			continue
		if y1 is not None and y2 is not None:
			year = int(file_[:-3])
			if not (year >= y1 and year <= y2):
				continue
		filename = os.path.join(input_, file_)
		images, labels, time0, lat0, lon0 = read_samples(filename)
		if night:
			images[:,:,:,0] = 0
		model = keras.models.load_model(model_file,
			custom_objects={'loss_func': get_loss_func(nclasses)})
		stats0 = model.predict(images)
		assert stats0.shape[-1] == 2*nclasses
		output_filename = os.path.join(output, file_)
		print('-> %s' % output_filename)
		ds.write(output_filename, {
			'time': time0,
			'stats': stats0[:,:,:,:nclasses],
			'lat': lat0,
			'lon': lon0,
			'.': {
				'time': {
					'.dims': ['sample'],
					'long_name': 'time',
					'units': 'days since -4713-11-24 12:00 UTC',
					'calendar': 'proleptic_gregorian',
				},
				'stats': {
					'.dims': ['sample', 'x', 'y', 'cloud_type'],
					'long_name': 'label statistics',
				},
				'lat': {
					'.dims': ['sample', 'x', 'y'],
					'long_name': 'latitude',
					'units': 'degrees_north',
				},
				'lon': {
					'.dims': ['sample', 'x', 'y'],
					'long_name': 'longitude',
					'units': 'degrees_east',
				},
			}
		})

def read_dataset(input_):
	images = []
	labels = []
	for file_ in sorted(os.listdir(input_)):
		if not file_.endswith('.nc'):
			continue
		filename = os.path.join(input_, file_)
		images0, labels0, _, _, _ = read_samples(filename)
		images += [images0]
		labels += [labels0]
	images = np.concatenate(images)
	labels = np.concatenate(labels)
	return images, labels

def train(input_, input_val, output, output_history, nclasses, night=False,
	inmemory=True):

	if inmemory:
		train_images, train_labels = read_dataset(input_)
		test_images, test_labels = read_dataset(input_val)
	else:
		datagen = DataGenerator(input_, batch_size=1)
		datagen_val = DataGenerator(input_val, batch_size=1)

	policy = tf.keras.mixed_precision.Policy("float64")
	tf.keras.mixed_precision.set_global_policy(policy)

	input_ = Input((16, 16, 2))
	model = unet_model(input_, nclasses, 16, 0.1, True)

	model.compile(optimizer='adam',
				  loss=get_loss_func(nclasses),
				  metrics=['accuracy'])

	callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

	if inmemory:
		history = model.fit(train_images, train_labels,
			epochs=40,
			validation_data=(test_images, test_labels),
			callbacks=[callback],
		)
	else:
		history = model.fit(datagen,
			epochs=40,
			validation_data=datagen_val,
			callbacks=[callback],
		)

	print('-> %s' % output)
	model.save(output)

	ds.write(output_history, {
		'loss': np.array(history.history['loss']),
		'val_loss': np.array(history.history['val_loss']),
		'.': {
			'loss': {'.dims': ['round']},
			'val_loss': {'.dims': ['round']},
		},
	})

if __name__ == '__main__':
	args, opts = pst.decode_argv(sys.argv, as_unicode=True)
	if len(args) < 2:
		sys.stderr.write(sys.modules[__name__].__doc__)
		sys.exit(1)
	action = args[1]
	night = opts.get('night', False)
	classes = opts.get('classes', 0)
	inmemory = opts.get('inmemory', True)

	tf.config.threading.set_inter_op_parallelism_threads(24)

	nclasses = {0: 4, 1: 10, 2: 27}[classes]

	if action == 'train':
		if len(args) != 6:
			sys.stderr.write(sys.modules[__name__].__doc__)
			sys.exit(1)
		input_ = args[2]
		input_val = args[3]
		output = args[4]
		output_history = args[5]
		train(input_, input_val, output, output_history, nclasses, night=night,
			inmemory=inmemory)
	elif action == 'apply':
		if len(args) != 7:
			sys.stderr.write(sys.modules[__name__].__doc__)
			sys.exit(1)
		model = args[2]
		input_ = args[3]
		y1 = args[4]
		y2 = args[5]
		output = args[6]
		apply_(model, input_, y1, y2, output, nclasses, night=night)
	else:
		raise ValueError('Unknown action "%s"' % action)
