#!/usr/bin/env python3
'''Train or apply a TensorFlow CNN.

Usage: tf train INPUT INPUT_VAL OUTPUT OUTPUT_HISTORY [OPTIONS]
       tf apply MODEL INPUT OUTPUT [OPTIONS]

Arguments (tf train):

  INPUT           Input directory with samples. The output of prepare_samples (NetCDF).
  INPUT_VAL       Input directory with validation samples (NetCDF).
  OUTPUT          Output model (HDF5).
  OUTPUT_HISTORY  History output (NetCDF).

Arguments (tf apply):

  MODEL   TensorFlow model (HDF5).
  INPUT   Input directory with samples. The output of prepare_samples (NetCDF).
  OUTPUT  Output samples directory (NetCDF).

Options (tf train):

  night: VALUE          Train for nighttime only. One of: true or false. Default: false.
  exclude_night: VALUE  Exclude nighttime samples. One of: true or false. Default: true.
  classes: VALUE        Classification. One of: 0 (4 cloud types), 1 (10 cloud genera), 2 (27 cloud genera). Default: 0.
  exclude: { LAT1 LAT2 LON1 LON2 }
      Exclude samples with pixels in a region bounded by given latitude and longitude. Default: none.
  nsamples: VALUE       Maximum number of samples to use for the training per day. Default: 20.

Options (tf apply):

  classes: VALUE  Classification. One of: 0 (4 cloud types), 1 (10 cloud genera), 2 (27 cloud genera). Default: 0.

Examples:

bin/tf train data/samples/ceres_training/training data/samples/ceres_training/validation data/ann/ceres.h5 data/ann/history.nc
bin/tf apply data/ann/ceres.h5 data/samples/ceres data/samples_pred/ceres
bin/tf apply data/ann/ceres.h5 data/samples/historical/AWI-ESM-1-1-LR data/samples_pred/historical/AWI-ESM-1-1-LR
'''

import warnings
warnings.filterwarnings('ignore')

import sys
import re
import os
import random
import numpy as np
import ds_format as ds
import aquarius_time as aq
import matplotlib.pyplot as plt
import pst
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Model
from keras.layers.merge import concatenate
from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, \
	AveragePooling2D, MaxPooling2D, Flatten, Dense, Dropout, \
	BatchNormalization, Activation
from tensorflow.keras.utils import Sequence

NSAMPLES = 20

META = {
	'time': {
		'.dims': ['sample'],
		'long_name': 'time',
		'units': 'days since -4713-11-24 12:00 UTC',
		'calendar': 'proleptic_gregorian',
	},
	'stats': {
		'.dims': ['sample', 'x', 'y', 'cloud_type'],
		'long_name': 'label statistics',
	},
	'lat': {
		'.dims': ['sample', 'x', 'y'],
		'long_name': 'latitude',
		'units': 'degrees_north',
	},
	'lon': {
		'.dims': ['sample', 'x', 'y'],
		'long_name': 'longitude',
		'units': 'degrees_east',
	},
}

def calc_stats_4(x, xn):
	shape = x.shape
	y = np.zeros((shape[0], 4, shape[2], shape[3]), x.dtype)
	yn = np.zeros((shape[0], 4, shape[2], shape[3]), x.dtype)
	y[:,0,:,:] = np.sum(x[:,0:9,:,:], axis=1) # High
	y[:,1,:,:] = np.sum(x[:,9:18,:,:], axis=1) # Middle
	y[:,2,:,:] = np.sum(x[:,18:21,:,:], axis=1) + x[:,25,:,:] + x[:,26,:,:] # Cumuliform
	y[:,3,:,:] = np.sum(x[:,21:25,:,:], axis=1) # Stratiform
	yn[:,0,:,:] = xn[:,0,:,:] # High
	yn[:,1,:,:] = xn[:,9,:,:] # Middle
	yn[:,2,:,:] = xn[:,18,:,:] # Cumuliform
	yn[:,3,:,:] = xn[:,18,:,:] # Stratiform
	return y, yn

def calc_stats_10(x, xn):
	shape = x.shape
	y = np.zeros((shape[0], 10, shape[2], shape[3]), x.dtype)
	yn = np.zeros((shape[0], 10, shape[2], shape[3]), x.dtype)
	y[:,0,:,:] = np.sum(x[:,0:6,:,:], axis=1) # Ci
	y[:,1,:,:] = np.sum(x[:,6:8,:,:], axis=1) # Cs
	y[:,2,:,:] = x[:,8,:,:] # Cc
	y[:,3,:,:] = np.sum(x[:,9:11,:,:], axis=1) # As
	y[:,4,:,:] = np.sum(x[:,11:18,:,:], axis=1) # Ac
	y[:,5,:,:] = np.sum(x[:,18:21,:,:], axis=1) # Cu
	y[:,6,:,:] = np.sum(x[:,21:23,:,:], axis=1) # Sc
	y[:,7,:,:] = np.sum(x[:,23:25,:,:], axis=1) # St
	y[:,8,:,:] = x[:,25,:,:] # Cu + Sc
	y[:,9,:,:] = x[:,26,:,:] # Cb
	yn[:,0,:,:] = xn[:,0,:,:] # Ci
	yn[:,1,:,:] = xn[:,0,:,:] # Cs
	yn[:,2,:,:] = xn[:,0,:,:] # Cc
	yn[:,3,:,:] = xn[:,9,:,:] # As
	yn[:,4,:,:] = xn[:,9,:,:] # Ac
	yn[:,5,:,:] = xn[:,18,:,:] # Cu
	yn[:,6,:,:] = xn[:,18,:,:] # Sc
	yn[:,7,:,:] = xn[:,18,:,:] # St
	yn[:,8,:,:] = xn[:,18,:,:] # Cu + Sc
	yn[:,9,:,:] = xn[:,18,:,:] # Cb
	return y, yn

def get_loss_func(nclasses):
	def loss_func(y_true, y_pred):
		a = y_true[:,:,:,0:nclasses]
		b = y_true[:,:,:,nclasses:(2*nclasses)]
		c = y_pred[:,:,:,0:nclasses]
		d = a*tf.experimental.numpy.log(c) + \
			(b - a)*tf.experimental.numpy.log(1 - c)
		x = -tf.experimental.numpy.sum(d)
		return x
	return loss_func

def unet_encoder(x, n, kernel_size=3, batchnorm=True, dropout=0, maxpool=True):
	y = Conv2D(n, (kernel_size, kernel_size), kernel_initializer='he_normal', \
		padding='same')(x)
	if batchnorm:
		y = BatchNormalization()(y)
	y = Activation('relu')(y)
	y = Conv2D(n, (kernel_size, kernel_size), kernel_initializer='he_normal', \
		padding='same')(x)
	if batchnorm:
		y = BatchNormalization()(y)
	y = Activation('relu')(y)
	z = y
	if maxpool:
		y = AveragePooling2D((2, 2))(y)
	if dropout > 0:
		y = Dropout(dropout)(y)
	return y, z

def unet_decoder(x, z, n, kernel_size=3, batchnorm=True, dropout=0):
	y = Conv2DTranspose(n, (kernel_size, kernel_size), strides=(2, 2),
		padding='same')(x)
	y = concatenate([y, z])
	y = Dropout(dropout)(y)
	y, _ = unet_encoder(y, n, kernel_size, batchnorm, 0, False)
	return y

def unet_model(x, nclasses, n=16, dropout=0.1, batchnorm=True):
	y, c1 = unet_encoder(x, n*1, 3, batchnorm, dropout)
	y, c2 = unet_encoder(y, n*2, 3, batchnorm, dropout)
	y, c3 = unet_encoder(y, n*4, 3, batchnorm, dropout)
	y, c4 = unet_encoder(y, n*8, 3, batchnorm, dropout)
	y, c5 = unet_encoder(y, n, 3, batchnorm, 0, False)
	y = unet_decoder(y, c4, n*8, 3, batchnorm, dropout)
	y = unet_decoder(y, c3, n*4, 3, batchnorm, dropout)
	y = unet_decoder(y, c2, n*2, 3, batchnorm, dropout)
	y = unet_decoder(y, c1, n*2, 3, batchnorm, dropout)
	y = Conv2D(nclasses*2, (1, 1), activation='sigmoid')(y)
	return Model(inputs=[x], outputs=[y])

def read_samples(filename, exclude=None, exclude_night=True, classes=0):
	print('<- %s' % filename)
	d = ds.read(filename, ['data', 'time', 'lat', 'lon', 'station_clouds', \
		'station_clouds_n'])
	if exclude is not None:
		mask = \
			(d['lat'] > exclude[0]) & \
			(d['lat'] <= exclude[1]) & \
			(d['lon'] > exclude[2]) & \
			(d['lon'] <= exclude[3])
		for i in range(d['station_clouds'].shape[1]):
			d['station_clouds'][:,i,:,:][mask] = 0
			d['station_clouds_n'][:,i,:,:][mask] = 0

	mask = np.all(d['station_clouds_n'] == 0, axis=(1, 2, 3))
	ds.select(d, {'sample': ~mask})

	if exclude_night:
		mask = ~np.any(np.isnan(d['data']), axis=(1, 2, 3))
		ds.select(d, {'sample': mask})
	else:
		d['data'][np.isnan(d['data'])] = -1
	n, m, l = d['data'].shape[2], d['data'].shape[3], d['data'].shape[0]
	images = np.moveaxis(d['data'], 1, 3)
	if 'station_clouds' in d:
		calc_stats = {
			0: calc_stats_4,
			1: calc_stats_10,
			2: lambda x, xn: (x, xn)
		}[classes]
		station_clouds, station_clouds_n = calc_stats(
			d['station_clouds'],
			d['station_clouds_n']
		)
		nclasses = station_clouds.shape[1]
		labels = np.full((l, n, m, nclasses*2), np.nan, np.float64)
		labels[:,:,:,:nclasses] = np.moveaxis(station_clouds, 1, 3)
		labels[:,:,:,nclasses:] = np.moveaxis(station_clouds_n, 1, 3)
		labels[np.isnan(labels)] = 0
	else:
		labels = None
	#mask = np.array([
	#	np.sum(np.isnan(d['data'][i,:,:])) == 0
	#	for i in range(l)
	#])
	mask = np.array([
		True
		for i in range(l)
	])
	return \
		images[mask,::], \
		labels[mask,::] if labels is not None else None, \
		d['time'][mask] if 'time' in d else None, \
		d['lat'][mask,::] if 'lat' in d else None, \
		d['lon'][mask,::] if 'lon' in d else None

def apply_(model_file, input_, output, classes=0, night=False,
	exclude_night=True):

	nclasses = {0: 4, 1: 10, 2: 27}[classes]

	files = os.listdir(input_)
	for file_ in sorted(files):
		if not file_.endswith('.nc'):
			continue
		filename = os.path.join(input_, file_)
		images, labels, time0, lat0, lon0 = read_samples(
			filename,
			classes=classes,
		)
		if night:
			images[:,:,:,0] = 0
		model = keras.models.load_model(model_file,
			custom_objects={'loss_func': get_loss_func(nclasses)})
		stats0 = model.predict(images)
		assert stats0.shape[-1] == 2*nclasses
		output_filename = os.path.join(output, file_)

		print('-> %s' % output_filename)
		ds.write(output_filename, {
			'time': time0,
			'stats': stats0[:,:,:,:nclasses],
			'lat': lat0,
			'lon': lon0,
			'.': META,
		})

def read_dataset(input_, exclude=None, exclude_night=True, classes=0,
	nsamples=NSAMPLES):
	images = []
	labels = []
	for file_ in sorted(os.listdir(input_)):
		if not file_.endswith('.nc'):
			continue
		filename = os.path.join(input_, file_)
		images0, labels0, _, _, _ = read_samples(
			filename,
			exclude=exclude,
			exclude_night=exclude_night,
			classes=classes,
		)
		if images0.shape[0] > nsamples:
			images0 = images0[:nsamples]
			labels0 = labels0[:nsamples]
		images += [images0]
		labels += [labels0]
	images = np.concatenate(images)
	labels = np.concatenate(labels)
	return images, labels

def train(input_, input_val, output, output_history, classes=0, night=False,
	exclude=None, exclude_night=True, nsamples=NSAMPLES):

	nclasses = {0: 4, 1: 10, 2: 27}[classes]

	train_images, train_labels = read_dataset(input_,
		exclude=exclude, exclude_night=exclude_night, classes=classes,
		nsamples=nsamples)
	test_images, test_labels = read_dataset(input_val,
		exclude=exclude, exclude_night=exclude_night, classes=classes,
		nsamples=nsamples)

	policy = tf.keras.mixed_precision.Policy("float64")
	tf.keras.mixed_precision.set_global_policy(policy)

	input_ = Input((16, 16, 2))
	model = unet_model(input_, nclasses, 16, 0.1, True)

	model.compile(optimizer='adam',
				  loss=get_loss_func(nclasses),
				  metrics=['accuracy'])

	callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

	history = model.fit(train_images, train_labels,
		epochs=40,
		validation_data=(test_images, test_labels),
		callbacks=[callback],
	)

	print('-> %s' % output)
	model.save(output)

	ds.write(output_history, {
		'loss': np.array(history.history['loss']),
		'val_loss': np.array(history.history['val_loss']),
		'.': {
			'loss': {'.dims': ['round']},
			'val_loss': {'.dims': ['round']},
		},
	})

if __name__ == '__main__':
	args, opts = pst.decode_argv(sys.argv, as_unicode=True)
	if len(args) < 2:
		sys.stderr.write(sys.modules[__name__].__doc__)
		sys.exit(1)
	action = args[1]
	night = opts.get('night', False)
	exclude_night = opts.get('exclude_night', True)
	classes = opts.get('classes', 0)
	exclude = opts.get('exclude')
	nsamples = opts.get('nsamples', NSAMPLES)

	tf.config.threading.set_inter_op_parallelism_threads(24)

	if action == 'train':
		if len(args) != 6:
			sys.stderr.write(sys.modules[__name__].__doc__)
			sys.exit(1)
		input_ = args[2]
		input_val = args[3]
		output = args[4]
		output_history = args[5]
		train(input_, input_val, output, output_history,
			classes=classes,
			night=night,
			exclude=exclude,
			exclude_night=exclude_night,
			nsamples=nsamples,
		)
	elif action == 'apply':
		if len(args) != 5:
			sys.stderr.write(sys.modules[__name__].__doc__)
			sys.exit(1)
		model = args[2]
		input_ = args[3]
		output = args[4]
		apply_(model, input_, output,
			classes=classes,
			night=night,
			exclude_night=exclude_night,
		)
	else:
		raise ValueError('Unknown action "%s"' % action)
