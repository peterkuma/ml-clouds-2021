#!/usr/bin/env python3
'''Train or apply the artificial neural network (ANN).

Usage: ann train INPUT INPUT_VAL OUTPUT OUTPUT_HISTORY [OPTIONS]
       ann apply MODEL INPUT OUTPUT [OPTIONS]

This program uses PST for command line argument parsing.

Arguments (ann train):

  INPUT           Input directory with samples. The output of prepare_samples (NetCDF).
  INPUT_VAL       Input directory with validation samples (NetCDF).
  OUTPUT          Output model (HDF5).
  OUTPUT_HISTORY  History output (NetCDF).

Arguments (ann apply):

  MODEL   TensorFlow model (HDF5).
  INPUT   Input directory with samples. The output of prepare_samples (NetCDF).
  OUTPUT  Output samples directory (NetCDF).

Options (ann train):

  night: VALUE          Train for nighttime only. One of: true or false. Default: false.
  exclude: { LAT1 LAT2 LON1 LON2 }
      Exclude samples with pixels in a region bounded by given latitude and longitude. Default: none.
  nsamples: VALUE       Maximum number of samples to use for the training per day. Default: 20.

Options:

  exclude_night: VALUE  Exclude nighttime samples. One of: true or false. Default: true.
  nclasses: VALUE  Number of cloud types. One of: 4, 10, 27. Default: 4.

Examples:

bin/ann train data/samples/ceres_training/training data/samples/ceres_training/validation data/ann/ceres.h5 data/ann/history.nc
bin/ann apply data/ann/ceres.h5 data/samples/ceres data/samples_pred/ceres
bin/ann apply data/ann/ceres.h5 data/samples/historical/AWI-ESM-1-1-LR data/samples_pred/historical/AWI-ESM-1-1-LR
'''

import warnings
warnings.filterwarnings('ignore')

import sys
import re
import os
import random
import numpy as np
import ds_format as ds
import aquarius_time as aq
import matplotlib.pyplot as plt
import pst
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Model
from keras.layers.merge import concatenate
from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, \
	AveragePooling2D, MaxPooling2D, Flatten, Dense, Dropout, \
	BatchNormalization, Activation
from tensorflow.keras.utils import Sequence

NSAMPLES = 20

META = {
	'time': {
		'.dims': ['sample'],
		'long_name': 'time',
		'units': 'days since -4713-11-24 12:00 UTC',
		'calendar': 'proleptic_gregorian',
	},
	'stats': {
		'.dims': ['sample', 'x', 'y', 'cloud_type'],
		'long_name': 'label statistics',
	},
	'lat': {
		'.dims': ['sample', 'x', 'y'],
		'long_name': 'latitude',
		'units': 'degrees_north',
	},
	'lon': {
		'.dims': ['sample', 'x', 'y'],
		'long_name': 'longitude',
		'units': 'degrees_east',
	},
}

def calc_stats_4(x, xn):
	shape = x.shape
	y = np.zeros((shape[0], 4, shape[2], shape[3]), x.dtype)
	yn = np.zeros((shape[0], 4, shape[2], shape[3]), x.dtype)
	y[:,0,:,:] = np.sum(x[:,0:9,:,:], axis=1) # High
	y[:,1,:,:] = np.sum(x[:,9:18,:,:], axis=1) # Middle
	y[:,2,:,:] = np.sum(x[:,18:21,:,:], axis=1) + x[:,25,:,:] + x[:,26,:,:] # Cumuliform
	y[:,3,:,:] = np.sum(x[:,21:25,:,:], axis=1) # Stratiform
	yn[:,0,:,:] = xn[:,0,:,:] # High
	yn[:,1,:,:] = xn[:,9,:,:] # Middle
	yn[:,2,:,:] = xn[:,18,:,:] # Cumuliform
	yn[:,3,:,:] = xn[:,18,:,:] # Stratiform
	return y, yn

def calc_stats_10(x, xn):
	shape = x.shape
	y = np.zeros((shape[0], 10, shape[2], shape[3]), x.dtype)
	yn = np.zeros((shape[0], 10, shape[2], shape[3]), x.dtype)
	y[:,0,:,:] = np.sum(x[:,0:6,:,:], axis=1) # Ci
	y[:,1,:,:] = np.sum(x[:,6:8,:,:], axis=1) # Cs
	y[:,2,:,:] = x[:,8,:,:] # Cc
	y[:,3,:,:] = np.sum(x[:,9:11,:,:], axis=1) # As
	y[:,4,:,:] = np.sum(x[:,11:18,:,:], axis=1) # Ac
	y[:,5,:,:] = np.sum(x[:,18:21,:,:], axis=1) # Cu
	y[:,6,:,:] = np.sum(x[:,21:23,:,:], axis=1) # Sc
	y[:,7,:,:] = np.sum(x[:,23:25,:,:], axis=1) # St
	y[:,8,:,:] = x[:,25,:,:] # Cu + Sc
	y[:,9,:,:] = x[:,26,:,:] # Cb
	yn[:,0,:,:] = xn[:,0,:,:] # Ci
	yn[:,1,:,:] = xn[:,0,:,:] # Cs
	yn[:,2,:,:] = xn[:,0,:,:] # Cc
	yn[:,3,:,:] = xn[:,9,:,:] # As
	yn[:,4,:,:] = xn[:,9,:,:] # Ac
	yn[:,5,:,:] = xn[:,18,:,:] # Cu
	yn[:,6,:,:] = xn[:,18,:,:] # Sc
	yn[:,7,:,:] = xn[:,18,:,:] # St
	yn[:,8,:,:] = xn[:,18,:,:] # Cu + Sc
	yn[:,9,:,:] = xn[:,18,:,:] # Cb
	return y, yn

def get_loss_func(nclasses):
	def loss_func(y_true, y_pred):
		a = y_true[:,:,:,0:nclasses]
		b = y_true[:,:,:,nclasses:(2*nclasses)]
		c = y_pred[:,:,:,0:nclasses]
		d = a*tf.experimental.numpy.log(c) + \
			(b - a)*tf.experimental.numpy.log(1 - c)
		x = -tf.experimental.numpy.sum(d)
		return x
	return loss_func

def unet_encoder(x, n, kernel_size=3, batchnorm=True, dropout=0, maxpool=True):
	y = Conv2D(n, (kernel_size, kernel_size), kernel_initializer='he_normal', \
		padding='same')(x)
	if batchnorm:
		y = BatchNormalization()(y)
	y = Activation('relu')(y)
	y = Conv2D(n, (kernel_size, kernel_size), kernel_initializer='he_normal', \
		padding='same')(x)
	if batchnorm:
		y = BatchNormalization()(y)
	y = Activation('relu')(y)
	z = y
	if maxpool:
		y = AveragePooling2D((2, 2))(y)
	if dropout > 0:
		y = Dropout(dropout)(y)
	return y, z

def unet_decoder(x, z, n, kernel_size=3, batchnorm=True, dropout=0):
	y = Conv2DTranspose(n, (kernel_size, kernel_size), strides=(2, 2),
		padding='same')(x)
	y = concatenate([y, z])
	y = Dropout(dropout)(y)
	y, _ = unet_encoder(y, n, kernel_size, batchnorm, 0, False)
	return y

def unet_model(x, nclasses, n=16, dropout=0.1, batchnorm=True):
	y, c1 = unet_encoder(x, n*1, 3, batchnorm, dropout)
	y, c2 = unet_encoder(y, n*2, 3, batchnorm, dropout)
	y, c3 = unet_encoder(y, n*4, 3, batchnorm, dropout)
	y, c4 = unet_encoder(y, n*8, 3, batchnorm, dropout)
	y, c5 = unet_encoder(y, n, 3, batchnorm, 0, False)
	y = unet_decoder(y, c4, n*8, 3, batchnorm, dropout)
	y = unet_decoder(y, c3, n*4, 3, batchnorm, dropout)
	y = unet_decoder(y, c2, n*2, 3, batchnorm, dropout)
	y = unet_decoder(y, c1, n*2, 3, batchnorm, dropout)
	y = Conv2D(nclasses*2, (1, 1), activation='sigmoid')(y)
	return Model(inputs=[x], outputs=[y])

def read_samples(filename, exclude=None, exclude_night=True, nclasses=4,
	training=False):
	print('<- %s' % filename)
	d = ds.read(filename, ['data', 'time', 'lat', 'lon', 'station_clouds', \
		'station_clouds_n'])

	if training:
		d['station_clouds'] = d['station_clouds'].filled(0)
		d['station_clouds_n'] = d['station_clouds_n'].filled(0)

	if exclude is not None:
		mask = \
			(d['lat'] > exclude[0]) & \
			(d['lat'] <= exclude[1]) & \
			(d['lon'] > exclude[2]) & \
			(d['lon'] <= exclude[3])
		if training:
			for i in range(d['station_clouds'].shape[1]):
				d['station_clouds'][:,i,:,:][mask] = 0
				d['station_clouds_n'][:,i,:,:][mask] = 0

	if training:
		mask = np.all(d['station_clouds_n'] == 0, axis=(1, 2, 3))
		ds.select(d, {'sample': ~mask})

	if exclude_night:
		mask = ~np.any(np.isnan(d['data']), axis=(1, 2, 3))
		ds.select(d, {'sample': mask})
	else:
		d['data'][np.isnan(d['data'])] = -1

	n, m, l = d['data'].shape[2], d['data'].shape[3], d['data'].shape[0]
	images = np.moveaxis(d['data'], 1, 3)

	calc_stats = {
		4: calc_stats_4,
		10: calc_stats_10,
		27: lambda x, xn: (x, xn)
	}[nclasses]

	if training:
		station_clouds, station_clouds_n = calc_stats(
			d['station_clouds'],
			d['station_clouds_n']
		)
		nclasses = station_clouds.shape[1]
		labels = np.full((l, n, m, nclasses*2), np.nan, np.float64)
		labels[:,:,:,:nclasses] = np.moveaxis(station_clouds, 1, 3)
		labels[:,:,:,nclasses:] = np.moveaxis(station_clouds_n, 1, 3)
		labels[np.isnan(labels)] = 0
	else:
		labels = None

	return images, labels, d['time'], d['lat'], d['lon']

def apply_(model_file, input_, output, nclasses=4, night=False,
	exclude_night=True):

	files = os.listdir(input_)
	for file_ in sorted(files):
		if not file_.endswith('.nc'):
			continue
		filename = os.path.join(input_, file_)
		images, labels, time0, lat0, lon0 = read_samples(
			filename,
			nclasses=nclasses,
			exclude_night=exclude_night,
		)
		if night:
			images[:,:,:,0] = 0
		model = keras.models.load_model(model_file,
			custom_objects={'loss_func': get_loss_func(nclasses)})
		stats0 = model.predict(images)
		assert stats0.shape[-1] == 2*nclasses
		output_filename = os.path.join(output, file_)

		print('-> %s' % output_filename)
		ds.write(output_filename, {
			'time': time0,
			'stats': stats0[:,:,:,:nclasses],
			'lat': lat0,
			'lon': lon0,
			'.': META,
		})

def read_dataset(input_, exclude=None, exclude_night=True, nclasses=4,
	nsamples=NSAMPLES):
	images = []
	labels = []
	for file_ in sorted(os.listdir(input_)):
		if not file_.endswith('.nc'):
			continue
		filename = os.path.join(input_, file_)
		images0, labels0, time, _, _ = read_samples(
			filename,
			exclude=exclude,
			exclude_night=exclude_night,
			nclasses=nclasses,
			training=True,
		)
		for t in sorted(set(time)):
			ii = np.where(time == t)[0]
			ii = ii[:min(len(ii), nsamples)]
			images += [images0[ii,::]]
			labels += [labels0[ii,::]]
	images = np.concatenate(images)
	labels = np.concatenate(labels)
	return images, labels

def train(input_, input_val, output, output_history, nclasses=4, night=False,
	exclude=None, exclude_night=True, nsamples=NSAMPLES):

	train_images, train_labels = read_dataset(input_,
		exclude=exclude, exclude_night=exclude_night, nclasses=nclasses,
		nsamples=nsamples)
	test_images, test_labels = read_dataset(input_val,
		exclude=exclude, exclude_night=exclude_night, nclasses=nclasses,
		nsamples=nsamples)

	policy = tf.keras.mixed_precision.Policy("float64")
	tf.keras.mixed_precision.set_global_policy(policy)

	input_ = Input((16, 16, 2))
	model = unet_model(input_, nclasses, 16, 0.1, True)

	model.compile(optimizer='adam',
				  loss=get_loss_func(nclasses),
				  metrics=['accuracy'])

	callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

	history = model.fit(train_images, train_labels,
		epochs=40,
		validation_data=(test_images, test_labels),
		callbacks=[callback],
	)

	print('-> %s' % output)
	model.save(output)

	ds.write(output_history, {
		'loss': np.array(history.history['loss']),
		'val_loss': np.array(history.history['val_loss']),
		'.': {
			'loss': {'.dims': ['round']},
			'val_loss': {'.dims': ['round']},
		},
	})

if __name__ == '__main__':
	args, opts = pst.decode_argv(sys.argv, as_unicode=True)
	if len(args) < 2:
		sys.stderr.write(sys.modules[__name__].__doc__)
		sys.exit(1)
	action = args[1]
	night = opts.get('night', False)
	exclude_night = opts.get('exclude_night', True)
	nclasses = opts.get('nclasses', 4)
	exclude = opts.get('exclude')
	nsamples = opts.get('nsamples', NSAMPLES)

	tf.config.threading.set_inter_op_parallelism_threads(24)

	if action == 'train':
		if len(args) != 6:
			sys.stderr.write(sys.modules[__name__].__doc__)
			sys.exit(1)
		input_ = args[2]
		input_val = args[3]
		output = args[4]
		output_history = args[5]
		train(input_, input_val, output, output_history,
			nclasses=nclasses,
			night=night,
			exclude=exclude,
			exclude_night=exclude_night,
			nsamples=nsamples,
		)
	elif action == 'apply':
		if len(args) != 5:
			sys.stderr.write(sys.modules[__name__].__doc__)
			sys.exit(1)
		model = args[2]
		input_ = args[3]
		output = args[4]
		apply_(model, input_, output,
			nclasses=nclasses,
			night=night,
			exclude_night=exclude_night,
		)
	else:
		raise ValueError('Unknown action "%s"' % action)
